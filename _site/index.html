<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Tom Alcorn&#39;s Blog</title>
  <meta name="description" content="Math, machine learning, and generative art.">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/">
  <link rel="alternate" type="application/rss+xml" title="Tom Alcorn&#39;s Blog" href="/feed.xml">
  <link rel="shortcut icon" type="image/png" href="/favicon.png">

  <link href="https://fonts.googleapis.com/css?family=Roboto|Roboto+Mono:400|Rubik:400,900" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script> -->
  
  
</head>


  <body>

    <!-- <div class="hero" style="background-image: url(/assets/img/small-wreck-tom.jpg)"> -->
    
      <div class="hero" style="background-image: url(/assets/img/wave-whale.jpg)">
        <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <div class="header-wrapper">
      <div class="site-title-box">
        <div class="site-title-wrapper">
          <a class="site-title" href="/">Tom Alcorn&#39;s Blog</a>
        </div>
      </div>

      

    </div>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="header-trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
            <a class="page-link" href="/papers/">Papers</a>
            
          
            
            
            <a class="page-link" href="/posts/">Posts</a>
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>

      </div>
    

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="with-sidebar">
          <div class="with-sidebar-content">
            <div class="home">

  


  <h2 class='latest-post'>Latest post:</h2>

  <!-- <ul class="post-list"> -->
    
    
    
    <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">
      <a href="/2017/12/30/seeing-like-a-perceptron-ii-training.html">Seeing Like A Perceptron II: Training</a>
    </h1>
    <p class="post-meta">
      <time datetime="2017-12-31T15:59:53+11:00" itemprop="datePublished">
        
        Dec 30, 2017
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This is a follow-up from a <a href="http://localhost:4000/2017/12/17/seeing-like-a-perceptron.html">previous post</a> in which I discussed how multi-layer perceptrons learn non-linear transformations that embed the input data into a new space in which the classes are linearly separable. The visualisations in that post can give you a clear picture of how the transformation works once the hidden layer perceptrons have already been trained, but what does the output layer see while training is still ongoing, and how does that affect the training process itself?</p>

<p>The other day I was playing around with a neural network implementation comparing my results with the scikit-learn MLPClassifier implementation to get a sense for which tricks and optimisations are most effective. I plotted training accuracy at each epoch to monitor my network’s progress as it trained. If you’ve ever trained a neural network you’ve probably used such a plot. They typically look something like this:</p>

<p><img src="http://localhost:4000/assets/2017-12-30-seeing-like-a-perceptron-ii-training/training_accuracy.png" alt="png" /></p>

<p>One key feature of these plots that shows up again and again is the cliff and plateau pattern: as the network trains it spends a lot of time making very little progress, and then all of a sudden it finds some new trick and the accuracy shoots up to a new higher plateau. So I began to wonder: what is going on in these plateaus? And is there anything I can do to “fast-forward” through them to the good bits, the cliffs, where the network seems to learn rapidly? To explore these questions, I’ve made a visualisation of the hidden layer outputs after each training epoch. In this classification problem, the initial dataset is two dimensional, and has two classes, one containing everything close to the origin (class 0) and another containing everything else (class 1):</p>

<p><img src="http://localhost:4000/assets/2017-12-30-seeing-like-a-perceptron-ii-training/original_data.png" alt="png" /></p>

<p>The network I’m training has a hidden layer with 3 nodes, and an output layer with 1 node that classifies the data as 0 or 1. Below there are two plots. On the left is a visualisation of what the output node “sees” as input after a given epoch, which are the outputs of the 3 hidden layer nodes. Blue points are class 0 and red points are class 1. On the right is a plot of the training accuracy at each epoch, with a marker to indicate the training accuracy for the chosen epoch. Try moving the epoch slider to see how the hidden layer transformation changes as the network learns.</p>

<div id="container">
    <head>
        <link rel="stylesheet" src="https://cdn.pydata.org/bokeh/release/bokeh-0.12.9.min.css" />
        <script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-0.12.9.min.js"></script>
        <script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-api-0.12.9.min.js"></script>
        <link rel="stylesheet" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.css" />
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/vis/4.21.0/vis.min.js"></script>
        <script type="text/javascript" src="http://localhost:4000/assets/lib/perceptrons.js"></script>
    </head>
    <style>
        #graph {
            display: flex;
            flex-direction: row;
            justify-content: space-around;
        }
        #controls {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
    </style>
    <div id="controls">
        <label>Epoch: </label><input style="width:40%;" id="epoch" type="range" min="0" max="999" value="290" step="1" oninput="updatePlot();" /> <label id="epochValue">290</label>
    </div>
    <div id="graph">
        <div id="transformedDataPlot"></div>
        <div id="trainingAccuracyPlot"></div>
    </div>
    <script type="text/javascript">
        perceptron = perceptronGenerator(relu);
        var epoch = 290;
        var data = new vis.DataSet();
        data.add({x:0,y:0,z:0,style:0});
        var options = {
            width:  '300px',
            height: '300px',
            style: 'dot-color',
            showPerspective: true,
            showGrid: true,
            showLegend: false,
            // keepAspectRatio: true,
            verticalRatio: 1.2,
            xMin: 0,
            xMax: 1.5,
            yMin: 0,
            yMax: 1.5,
            zMin: 0,
            zMax: 0.4,
            // xLabel: 'Perceptron #0 output',
            // yLabel: 'Perceptron #1 output',
            // zLabel: 'Perceptron #2 output',
            cameraPosition: {
                horizontal: 1.1,
                vertical: 0,
                distance: 1.9,
            },
        };
        var graph = new vis.Graph3d(document.getElementById('transformedDataPlot'), data, options);
        // create some ranges for the plot
        var xdr = new Bokeh.Range1d({ start: 0, end: 1000 });
        var ydr = Bokeh.Range1d(0.5, 1);
        // make the plot
        var accPlot = new Bokeh.Plot({
            x_range: xdr,
            y_range: ydr,
            sizing_mode: 'scale_width',
            // plot_width: 400,
            // plot_height: 400,
            background_fill_color: "#fdfdfd"
        });
        var xaxis = new Bokeh.LinearAxis({ axis_line_color: null, axis_label: "Epoch" });
        var yaxis = new Bokeh.LinearAxis({ axis_line_color: null, axis_label: "Training accuracy" });
        accPlot.add_layout(xaxis, "below");
        accPlot.add_layout(yaxis, "left");
        var doc = new Bokeh.Document();
        doc.add_root(accPlot);
        var div = document.getElementById("trainingAccuracyPlot");
        Bokeh.embed.add_document_standalone(doc, div);
        var updatePlot;
        var epochMarker;
        fetch('http://localhost:4000/assets/data/data_0.json').then((response) => response.json())
        .then((data0) => {
            fetch('http://localhost:4000/assets/data/data_1.json').then((response) => response.json())
            .then((data1) => {
                fetch('http://localhost:4000/assets/data/weights.json').then((response) => response.json())
                .then((weights) => {
                    fetch('http://localhost:4000/assets/data/accuracy.json').then((response) => response.json())
                    .then((accuracy) => {
                        accData = new Bokeh.ColumnDataSource({
                            data: {
                                x: range(1000),
                                y: accuracy,
                            },
                        });
                        accPlot.add_glyph(new Bokeh.Line({
                                x: { field: "x" },
                                y: { field: "y" },
                                line_color: "#040273",
                                line_width: 1,
                            }),
                            accData
                        );
                        epochMarker = new Bokeh.ColumnDataSource({
                            data: {
                                x: [epoch],
                                y: [accuracy[epoch]],
                            },
                        });
                        accPlot.add_glyph(new Bokeh.Circle({
                            x: { field: "x" },
                            y: { field: "y" },
                            line_color: "#0000ff",
                            fill_color: "#0000ff",
                            size: 10,
                            }), 
                            epochMarker
                        );
                        plot(data0, data1, weights, accuracy, epoch);
                        updatePlot = function() {
                            epoch = parseInt(document.getElementById('epoch').value);
                            document.getElementById('epochValue').innerText = epoch;
                            plot(data0, data1, weights, accuracy, epoch);
                        }
                    });
                });
            });
        });
        function plot(data0, data1, weights, accuracy, epoch) {
            var d = new vis.DataSet();
            var perceptrons = weights[epoch];
            var dt0 = transform(perceptrons, data0);
            var dt1 = transform(perceptrons, data1);
            dt0.map((x) => d.add({x:x[0], y:x[1], z:x[2], style:0}));
            dt1.map((x) => d.add({x:x[0], y:x[1], z:x[2], style:1}));
            graph.setData(d);
            epochMarker.data = {
                x: [epoch],
                y: [accuracy[epoch]],
            };
        }
    </script>
</div>
<p><br />
Recall that the function of the output node is to find a decision boundary (in this case a 2D plane) through this space that cleanly separates the data into the two classes. So if the red and blue points are all jumbled together, the output node will not be able to find a separating plane and the training accuracy will be low. Conversely, if the red and blue points are spread out and clustered among their own class, the output node should be able find a good decision boundary and the training accuracy will be high.</p>

<p>In the beginning, at epoch 0, the network has been randomly initialised with small weights taken from the normal distribution, so the hidden layer outputs are all clustered around the origin. The training accuracy plateaus until around epoch 140, at which point it shoots up significantly. However, looking at the hidden layer outputs during epochs 0 to 140, a lot of changes have been happening! The network first spread the data up along the z-axis, then rotated it to fall along a line in the x-y plane, and then finally discovered a new feature that shot a new strand of data up the z-axis. Just as it discovers this new feature, the training accuracy starts to ramp up significantly. And you can see why: around epoch 150 the blue points are mostly around the origin and the red points are scattered up the z-axis and into the x-y plane. You could imagine drawing a plane that shears off the corner of the plot where the origin is, so that red points are mostly above the plane and blue points are mostly below, trapped between it and the origin.</p>

<p>There is another training accuracy plateau from around epoch 160 to 280 in which the accuracy wiggles between 0.75 and 0.78. Again, this apparent steadiness belies some interesting changes that are happening in the hidden layer. The transformed data is being spread along all three axes now, pushing the red points out away from the origin but leaving the blue points close to it. At epoch 280 the results of this change start to appear in the training accuracy, and it shoots up again to stabilise around 0.95. The same decision boundary plane from before, cutting out the corner around the origin, works even better now. The training accuracy enters a final plateau, and the hidden layer continues to spread the data in the same directions. Perhaps if I trained the network long enough it would find a new way to transform the data to get the final 5% that it is failing to classify.</p>

<p>So what have we seen? Is there any way to fast-forward through the accuracy plateaus to speed up training? Unfortunately, no: when the training accuracy isn’t moving, the network is doing work that will be the foundation for the next rapid rise in accuracy. If we had found that in those plateaus the network is stuck in a rut, flailing around randomly and retrying the same missteps over and over, then perhaps we could help it along by introducing more randomness to “jump start” it, or by offering some guidance, e.g. by eliminating parts of the search space, or introducing new features or heuristics. But in this case, there is not much we can do.</p>

<p>This phenomena highlights the danger of placing too much emphasis on training accuracy when assessing the progress of the network. During any one of those plateaus, you might have started to believe that the network wasn’t working and decided to cut training short. If the network was actually making progress, you’d have mistakenly wasted all the training that had already gone into it. On the other hand, if the network really was stuck in a rut, continuing training would also be a waste of time. Is there a better metric that we can use to monitor our network as it learns?</p>

<p>Mean squared error is a good candidate: instead of just checking whether the network classified correctly or not, it accounts for how far off the prediction was. This is much more similar to the actual maximum likelihood error function that the network is minimising through gradient descent, but has the advantage of being cheap to compute at each epoch. Thus, the improvements that the network makes during those training accuracy plateaus are discernible in the mean-squared error, as you can see here:</p>

<p><img src="http://localhost:4000/assets/2017-12-30-seeing-like-a-perceptron-ii-training/mean_squared_error.png" alt="png" /></p>

<p>Therefore, and perhaps surprisingly, the mean squared error is a better choice for measuring training progress, even when the final metric you’re interested in is training accuracy.</p>

<p>Below I’ve provided my neural network implementation so that you can try this out for yourself. It’s a simple network with a single hidden layer, and it uses ReLU activations and sigmoid activation gradients. Most importantly, it keeps a history of all the weights it has learned and the accuracy and mean squared error at each epoch, so that you can play back the training process. Enjoy!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sigmoid_grad</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="p">)</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">/</span> <span class="mf">2.</span>

<span class="k">def</span> <span class="nf">relu_grad</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">NeuralNetworkWithPCA</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_input</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_hidden</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_output</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_input</span> <span class="o">=</span> <span class="n">num_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden</span> <span class="o">=</span> <span class="n">num_hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_output</span> <span class="o">=</span> <span class="n">num_output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="n">num_epochs</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_0_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_input</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">))</span>
        <span class="c"># randomly initialise hidden layer weights to start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_1_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_output</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation_function</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_0_1_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">num_input</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_1_2_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_output</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">_activation_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="n">funcs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">'relu'</span><span class="p">:</span> <span class="p">(</span><span class="n">relu</span><span class="p">,</span> <span class="n">relu_grad</span><span class="p">),</span>
            <span class="s">'sigmoid'</span><span class="p">:</span> <span class="p">(</span><span class="n">sigmoid</span><span class="p">,</span> <span class="n">sigmoid_grad</span><span class="p">),</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">funcs</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_0_1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_1_2</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">num_progress_bars</span> <span class="o">=</span> <span class="mi">20</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_output</span> <span class="o">=</span> <span class="n">error</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_hidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_1_2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dw_1_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_output</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dw_0_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="nb">input</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_hidden</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_1_2</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dw_1_2</span> <span class="o">/</span> <span class="n">n</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_0_1</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dw_0_1</span> <span class="o">/</span> <span class="n">n</span>
            
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_0_1_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_0_1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_1_2_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_1_2</span>

            <span class="n">finish</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">elapsed_time</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>

            <span class="n">correct</span> <span class="o">=</span> <span class="nb">len</span><span class="p">([</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">error</span> <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">])</span>
            <span class="n">training_accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">training_accuracy</span>
            
            <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">error</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mse</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mse</span>
            
            <span class="n">progress</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">*</span> <span class="n">num_progress_bars</span><span class="p">)</span>

            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">"</span><span class="se">\r</span><span class="s">"</span> <span class="o">+</span> <span class="s">" "</span><span class="o">*</span><span class="mi">80</span><span class="p">)</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">((</span><span class="s">"</span><span class="se">\r</span><span class="s">Training Accuracy: </span><span class="si">%0.3</span><span class="s">g</span><span class="si">%% </span><span class="s">Time elapsed: </span><span class="si">%0.2</span><span class="s">gs |"</span> <span class="o">+</span> <span class="s">"="</span><span class="o">*</span><span class="p">(</span><span class="n">progress</span><span class="p">)</span> <span class="o">+</span> <span class="s">" "</span><span class="o">*</span><span class="p">(</span><span class="n">num_progress_bars</span><span class="o">-</span><span class="n">progress</span><span class="p">)</span> <span class="o">+</span> <span class="s">"|"</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">training_accuracy</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">elapsed_time</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="nb">len</span><span class="p">([</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">error</span> <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">])</span>
        <span class="n">training_accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
        <span class="k">return</span> <span class="n">training_accuracy</span>
    
    <span class="k">def</span> <span class="nf">_mse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_history</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span><span class="p">)</span>
        <span class="n">mse</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">error</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
    
    <span class="k">def</span> <span class="nf">_layer_0_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">epoch</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_0_1_history</span><span class="p">[</span><span class="n">epoch</span><span class="p">]))</span>
    
    <span class="k">def</span> <span class="nf">_forward_history</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">epoch</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_0_1_history</span><span class="p">[</span><span class="n">epoch</span><span class="p">]))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_1_2_history</span><span class="p">[</span><span class="n">epoch</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>


  </div>

  
</article>

  <!-- </ul> -->

  <p class="rss-subscribe">subscribe <a href="/feed.xml">via RSS</a></p>

</div>

          </div>
          <div class="sidebar">
    
    
        <p>Math, machine learning, and generative art.</p>

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="sidebar-trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
            <a class="page-link" href="/papers/">Papers</a>
            
          
            
            
            <a class="page-link" href="/posts/">Posts</a>
            
          
            
            
          
        </div>
      </nav>
    

    
    <p>
      <a href="https://github.com/tdb-alcorn"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span></a>
    </p>
    
</div>
        </div>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!-- <h2 class="footer-heading">Tom Alcorn&#39;s Blog</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <p>Math, machine learning, and generative art.</p>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/tdb-alcorn"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">tdb-alcorn</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <ul class="contact-list">
            
            <li>
                tdbalcorn at gmail.com
            </li>
            
        </ul>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
