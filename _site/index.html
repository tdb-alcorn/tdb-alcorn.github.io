<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Tom Alcorn&#39;s Blog</title>
  <meta name="description" content="Math, machine learning, and generative art.">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/">
  <link rel="alternate" type="application/rss+xml" title="Tom Alcorn&#39;s Blog" href="/feed.xml">
  <link rel="shortcut icon" type="image/png" href="/favicon.png">

  <link href="https://fonts.googleapis.com/css?family=Roboto|Roboto+Mono:400|Rubik:400,900" rel="stylesheet">

  <!-- <link rel="stylesheet" href="https://unpkg.com/tachyons@4.9.0/css/tachyons.min.css"/> -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  
  
</head>


  <body>

    <!-- <div class="hero" style="background-image: url(/assets/img/small-wreck-tom.jpg)"> -->
    
      <div class="hero" style="background-image: url(/assets/img/wave-whale.jpg)">
        <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <div class="header-wrapper">
      <div class="site-title-box">
        <div class="site-title-wrapper">
          <a class="site-title" href="/">Tom Alcorn&#39;s Blog</a>
        </div>
      </div>

      

    </div>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="header-trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
            <a class="page-link" href="/papers/">Papers</a>
            
          
            
            
            <a class="page-link" href="/posts/">Posts</a>
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>

      </div>
    

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="with-sidebar">
          <div class="with-sidebar-content">
            <div class="home">

  


  <h2 class='latest-post'>Latest post:</h2>

  <!-- <ul class="post-list"> -->
    
    
    
    <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">
      <a href="/2017/12/17/seeing-like-a-perceptron.html">Seeing Like A Perceptron</a>
    </h1>
    <p class="post-meta">
      <time datetime="2017-12-18T10:59:53+11:00" itemprop="datePublished">
        
        Dec 17, 2017
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Perceptrons were one of the earliest supervised learning models, and though they went out of fashion for about 50 years, they’ve more recently experienced a huge comeback in the form of multi-layer perceptrons, also known as neural networks. You may have heard about how neural networks can now perform all kinds of marvelous and perhaps unsettling feats, such as <a href="https://research.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html">recognising speech</a>, <a href="https://cs.stanford.edu/people/karpathy/deepimagesent/">captioning images</a> and <a href="http://googleresearch.blogspot.com/2016/01/alphago-mastering-ancient-game-of-go.html">beating humans at games</a>. But they are surrounded by an unfortunate aura of mystery and magic, and I sometimes hear people complain that the study of neural networks lacks sound theoretical foundations. To debunk this idea, I’ll explore some visualisations of what a perceptron “sees” when it makes a decision, which will help us understand why chaining together multiple layers of perceptrons is such an effective technique.</p>

<p>First, some background: a perceptron is a type of linear model that learns a decision boundary by averaging together data points until it reaches a stable solution. I’ll only be considering perceptrons with two input nodes, but the intuition we’ll develop applies equally well to higher dimensional data, although it becomes substantially trickier to visualise. For our purposes, a trained perceptron is a function of an input vector <script type="math/tex">x</script> that produces a prediction <script type="math/tex">\hat{y}</script>, between 0 and 1. You will usually see this function written as
<script type="math/tex">\hat{y} = \sigma(Wx + b)</script>
where <script type="math/tex">\sigma</script> is the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid activation function</a>, <script type="math/tex">W</script> is a vector of weights connecting units in the network, <script type="math/tex">x</script> is the input vector (the data) and <script type="math/tex">b</script> is a bias term. However, the bias won’t qualitatively change the following visualisations so I am going to set all biases to 0 to keep things simple. So, in the 2 input case this breaks down as
<script type="math/tex">\hat{y} = \sigma(w_0 x_0 + w_1 x_1)</script>
Essentially, a perceptron takes a weighted average of its input nodes and then computes the sigmoid of the result, yielding a result between 0 and 1. Why the sigmoid? This may seem myterious and arbitrary to you right now, but we are going to pry open the black box to see why the sigmoid works as an activation function.</p>

<p>We’ll be thinking about the perceptron as a classifer, which means that we use the output of the perceptron to partition the input space into two classes, <strong>class #0</strong> and <strong>class #1</strong>. The line between the classes is the decision boundary, and it corresponds to a contour (line of constant value) of the perceptron function, typically something like <script type="math/tex">\hat{y}</script> = 0.5.</p>

<p>Ok, enough with the small talk, let’s start visualising stuff! This is our data, the ground truth that our model will try to learn:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="s">'retina'</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">40</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="n">data_0</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">data_1</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #1'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$x_0$"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/2017-12-17-seeing-like-a-perceptron/output_2_1.png" alt="png" /></p>

<p>And here is our neural network:</p>

<p><img src="http://localhost:4000/assets/2017-12-17-seeing-like-a-perceptron/network-diagram.svg" alt="network-diagram.svg" /></p>

<p>It has 2 inputs, 1 output, and a hidden layer with 2 perceptron units in the middle. And here are the two decision boundaries that the hidden layer perceptrons have found:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">perceptron</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">decision_boundary</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">*</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x0</span>

<span class="n">w0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #1'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="n">decision_boundary</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">t0</span><span class="p">),</span> <span class="s">'m'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Perceptron #0 decision boundary'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">decision_boundary</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">t1</span><span class="p">),</span> <span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Perceptron #1 decision boundary'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$x_0$"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/2017-12-17-seeing-like-a-perceptron/output_6_1.png" alt="png" /></p>

<p>Perceptron #0 classifies everything to the right of the line as 1 and everything to the left as 0, and perceptron #1 classifies everything above the line as 1 and everything below as 0. You can see that on their own, neither perceptron does a very good job of classifying the data, but there is a region, the upper right quadrant, in which they both get most of their predictions correct. If we could somehow combine these decision boundaries we might have a reasonably good classifier…</p>

<p>And this is precisely what a multi-layer perceptron does! We’ve made a second layer that combines the outputs of the first layer (the outputs of perceptrons #0 and #1) to make a better prediction than either classifier in the first layer could have independently come up with.</p>

<p>But as we know, a perceptron is a linear model: the only thing it can do is draw a line through the input space that hopefully separates the data into two classes. This means that if our second layer makes good predictions, the inputs that it received must have been linearly separable (or nearly so). Therefore, we can deduce that the first layer must have performed some kind of transformation on the original data that yielded a new set of data that was linearly separable! A natural question, then, is what exactly do the inputs to the second layer look like? Or, to put it more precisely, what does our input space look like after the first layer has applied its transformation?</p>

<p>To get an idea of what happens, let’s look at what happens to the gridlines in the input space once they’ve been transformed by the hidden layer:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nl_transform</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">perceptron</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">perceptron</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">x</span><span class="p">)],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="n">n_gridlines</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">gridline_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">n_gridlines</span><span class="p">)</span>
<span class="n">gridline_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">n_gridlines</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">glx</span> <span class="ow">in</span> <span class="n">gridline_x</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">glx</span><span class="p">,</span> <span class="n">gridline_y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:rust'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gly</span> <span class="ow">in</span> <span class="n">gridline_y</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gridline_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">gly</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:deep blue'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$x_0$"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">glx</span> <span class="ow">in</span> <span class="n">gridline_x</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">nl_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">glx</span><span class="p">,</span> <span class="n">gridline_y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:rust'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gly</span> <span class="ow">in</span> <span class="n">gridline_y</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">nl_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gridline_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">gly</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:deep blue'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Perceptron #0 output'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Perceptron #1 output'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/2017-12-17-seeing-like-a-perceptron/output_9_1.png" alt="png" /></p>

<p>Woah! Here the <script type="math/tex">y</script> (vertical) gridlines from the input space are plotted in red and the <script type="math/tex">x</script> (horizontal) gridlines are plotted in blue. So what does this tell us? First, notice that the entirety of our two-dimensional space (all of <script type="math/tex">\mathbb{R}^2</script>) has been mapped into a 1x1 square centred on the point (0.5,0.5). This is a consequence of the sigmoid function, which only outputs values between 0 and 1. Most of the new space is filled up with gridlines that were close to the decision boundaries in the input space. All the other gridlines (extending off to <script type="math/tex">\pm \infty</script> in both directions) have been squished up against the sides of the 1x1 square.</p>

<p>So what happened to our data?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nl_transform</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">perceptron</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">perceptron</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">x</span><span class="p">)],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="n">data_t</span> <span class="o">=</span> <span class="n">nl_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">data_t_0</span> <span class="o">=</span> <span class="n">data_t</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">data_t_1</span> <span class="o">=</span> <span class="n">data_t</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">db0</span> <span class="o">=</span> <span class="n">nl_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">t0</span><span class="p">,</span> <span class="n">decision_boundary</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">t0</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">db1</span> <span class="o">=</span> <span class="n">nl_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">t1</span><span class="p">,</span> <span class="n">decision_boundary</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">t1</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_t_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_t_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_t_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_t_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #1'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">db0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">db0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'m'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Perceptron #0 decision boundary'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">db1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">db1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Perceptron #1 decision boundary'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Perceptron #0 output'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Perceptron #1 output'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/2017-12-17-seeing-like-a-perceptron/output_11_1.png" alt="png" /></p>

<p>Holy smokes! We can immediately see that the data has been squished out to the sides of the plot and is now linearly separable, meaning that a perceptron that used this space as input would be able to draw a linear decision boundary that correctly classifies all the data. Additionally, something interesting has happened to the old decision boundaries: they have become like a set of axes (an orthogonal basis for the space). Could we have guessed that this would happen? It turns out we could have: remember that a decision boundary is just a line along which the perceptron predicts a constant value, called the threshold. For example, if our decision threshold is 0.5 then the decision boundary is the set of points along which the perceptron outputs <script type="math/tex">\hat{y}</script> = 0.5. Now, since the predictions of each perceptron make up the coordinates of our new space, we see the decision boundaries as lines of constant value in each coordinate positioned at the decision threshold.</p>

<p>Let’s see the data and the gridlines together:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_gridlines</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">gridline_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">n_gridlines</span><span class="p">)</span>
<span class="n">gridline_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">n_gridlines</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">glx</span> <span class="ow">in</span> <span class="n">gridline_x</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">glx</span><span class="p">,</span> <span class="n">gridline_y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:rust'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gly</span> <span class="ow">in</span> <span class="n">gridline_y</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gridline_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">gly</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:deep blue'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #0'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #1'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$x_0$"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">glx</span> <span class="ow">in</span> <span class="n">gridline_x</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">nl_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">glx</span><span class="p">,</span> <span class="n">gridline_y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:rust'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gly</span> <span class="ow">in</span> <span class="n">gridline_y</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">nl_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gridline_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">gly</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:deep blue'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_t_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_t_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #0'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_t_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_t_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #1'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Perceptron #0 output'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Perceptron #1 output'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/2017-12-17-seeing-like-a-perceptron/output_13_1.png" alt="png" /></p>

<p>Now it’s easy to see why all the data has become concentrated around the edges: that’s where the gridlines went too! We can think of this as a kind of fish-eye effect: the new space magnifies the parts of the old space which were adjacent to the decision boundaries (like the middle of a fish-eye lens) and pushes everything else out of the way. Why does this work for training neural networks? Loosely, you can think of the space around the decision boundaries as the most “interesting” parts of the data: it’s the space that the two perceptrons in the first layer are least confident about classifying, so it makes sense to enhance our view of that part of the space. Since a perceptron is more confident in its predictions the farther you get away from a decision boundary, the space far away from a decision boundary is boring and predictable, so we can safely ignore it.</p>

<p>In the abstract, we can think of this effect as being the result of some transformation applied to the input space, which just so happens to be useful for classifying the data <sup id="fnref:3blue1brown"><a href="#fn:3blue1brown" class="footnote">1</a></sup>. One of the best ways to understand a transformation is to play with it. In our case, the transformation is computed by the hidden layer perceptrons, and so there are 4 parameters to play with: <script type="math/tex">w_{0,0}, w_{0,1}</script> control the first perceptron and <script type="math/tex">w_{1,0}, w_{1,1}</script>. What happens when you change the weights of the hidden layer? Can you find other values for the weights that also yield linearly separable data? Try it out for yourself:</p>

<p><br /></p>

<script type="text/javascript" src="https://cdn.plot.ly/plotly-1.31.2.min.js"></script>

<div>
    <style>
        .perceptron-parameters {
            width: 30%;
            margin: auto;
            list-style: none;
        }
        ul > li > label {
            font-family: 'Roboto Mono', monospace;
            font-weight: 400;
        }
        ul > li > input[type=text] {
            font-size: 16px;
            font-family: 'Roboto Mono', monospace;
            font-weight: 400;
            width: 40px;
            border: 0;
            background-color: #fdfdfd;
        }
        #plot {
            width: 600px;
            margin: auto;
            height: 600px;
            padding-bottom: 10px;
        }
    </style>
    <ul class="perceptron-parameters">
        <li>
            <label>w<sub>0,0</sub> </label>
            <input id="w00" type="range" min="-10" max="10" value="10" step="0.1" oninput="updatePerceptrons(); updatePlot();" />
            <input type="text" id="w00Text" value="10" readonly="" />
        </li>
        <li>
            <label>w<sub>0,1</sub> </label>
            <input id="w01" type="range" min="-10" max="10" value="-1" step="0.1" oninput="updatePerceptrons(); updatePlot();" />
            <input type="text" id="w01Text" value="-1" readonly="" />
        </li>
        <li>
            <label>w<sub>1,0</sub> </label>
            <input id="w10" type="range" min="-10" max="10" value="-1" step="0.1" oninput="updatePerceptrons(); updatePlot();" />
            <input type="text" id="w10Text" value="-1" readonly="" />
        </li>
        <li>
            <label>w<sub>1,1</sub> </label>
            <input id="w11" type="range" min="-10" max="10" value="10" step="0.1" oninput="updatePerceptrons(); updatePlot();" />
            <input type="text" id="w11Text" value="10" readonly="" />
        </li>
    </ul>
    <div id="plot"></div>
</div>

<script type="text/javascript">
    function sigmoid(x) {
        return 1 / (1 + Math.exp(-1 * x));
    }

    function perceptron(weights, x) {
        return sigmoid(dot(weights, x));
    }

    function dot(x, y) {
        if (x.length !== y.length) {
            throw new Error("Length mismatch");
        }
        return x.map((_, i) => x[i] * y[i]).reduce((acc, val) => acc + val, 0);
    }

    function transform(perceptrons, inputs) {
        return inputs.map((x) => perceptrons.map((w) => perceptron(w, x)));
    }

    function range(n) {
        var r = Array(n);
        for (let i=0; i<n; i++) {
            r[i] = i;
        }
        return r;
    }

    function linspace(min, max, num) {
        var step = (max - min) / (num - 1);
        return range(num).map((s) => min + s * step);
    }

    function first(pair) {
        return pair[0];
    }

    function second(pair) {
        return pair[1];
    }

    var deepBlue = "#040273";
    var rust = "#a83c09";

    var numGridlines = 100;

    var xValues = linspace(-2.5, 2.5, numGridlines);
    var yValues = linspace(-1.5, 1.5, numGridlines);

    var perceptrons = [
        [10, -1],
        [-1, 10],
    ];

    var data1 = [
        [ 2.29249034,  0.48881005],
        [ 0.71026699,  1.05553444],
        [ 0.05407310,  0.25795342],
        [ 0.58828165,  0.88524424],
        [ 0.29349415,  0.10895031],
        [ 0.03172679,  1.27263986],
        [ 1.07144790,  0.41581801],
    ];
    var data0 = [
        [-0.60754770, -0.12613641],
        [-0.68460636,  0.92871475],
        [-1.84440103, -0.46700242],
        [-1.01700702, -0.13369303],
        [-0.43818550,  0.49344349],
        [-0.19900912, -1.27498361],
        [ 1.55067923, -0.31137892],
        [-1.37923991,  1.37140879],
        [ 0.02771165, -0.32039958],
        [-0.84617041, -0.43342892],
        [-1.33703450,  0.20917217],
        [-1.42432130, -0.55347685],
        [ 0.07479864, -0.50561983],
    ];

    function generateTraces(perceptrons, data0, data1, xValues, yValues) {
        var gridlinesX = yValues.map((y) => xValues.map((x) => [x, y]))
            .map((gx) => transform(perceptrons, gx))
            .map((gx) => {
                return {
                    x: gx.map(first),
                    y: gx.map(second),
                    mode: 'lines',
                    line: {
                        color: deepBlue,
                        width: 1,
                    },
                };
            });

        var gridlinesY = xValues.map((x) => yValues.map((y) => [x, y]))
            .map((gy) => transform(perceptrons, gy))
            .map((gy) => {
                return {
                    x: gy.map(first),
                    y: gy.map(second),
                    mode: 'lines',
                    line: {
                        color: rust,
                        width: 1,
                    },
                };
            });

        var td0 = transform(perceptrons, data0);
        var td1 = transform(perceptrons, data1);

        return [{
            x: td0.map(first),
            y: td0.map(second),
            mode: 'markers',
            marker: {
                color: "#0000ff",
                size: 10,
            },
        },
        {
            x: td1.map(first),
            y: td1.map(second),
            mode: 'markers',
            marker: {
                color: "#ff0000",
                size: 10,
            },
        }]
        .concat(gridlinesX).concat(gridlinesY);
    }

    var layout = {
        showlegend: false,
        // width: 600,
        // height: 600,
        autosize: true,
        position: 'center',
        xaxis: {
            showgrid: false,
            title: 'Perceptron #0 output',
        },
        yaxis: {
            showgrid: false,
            title: 'Perceptron #1 output',
        },
        margin: {
            t:20, r:20, b:40, l:40,
            pad: 0,
        },
    };

    var options = {
        displayModeBar: false,
        staticPlot: true,
    };

    Plotly.newPlot('plot', generateTraces(perceptrons, data0, data1, xValues, yValues), layout, options);

    function updatePerceptrons() {
        perceptrons[0][0] = document.getElementById("w00").value;
        perceptrons[0][1] = document.getElementById("w01").value;
        perceptrons[1][0] = document.getElementById("w10").value;
        perceptrons[1][1] = document.getElementById("w11").value;

        document.getElementById("w00Text").value = perceptrons[0][0];
        document.getElementById("w01Text").value = perceptrons[0][1];
        document.getElementById("w10Text").value = perceptrons[1][0];
        document.getElementById("w11Text").value = perceptrons[1][1];
    }

    function updatePlot() {
        var numTraces = xValues.length + yValues.length + 2; // 2 extra ones for the data scatter traces
        Plotly.deleteTraces('plot', range(numTraces));
        Plotly.addTraces('plot', generateTraces(perceptrons, data0, data1, xValues, yValues));
    }
</script>

<p>There is a lot you can learn from playing with these perceptrons, so I’ll just point out one interesting observation. You may have noticed that, in general, the bigger you make the weights (in absolute value) the “sharper” the fish-eye effect. In other words, the gridlines in the center of the space get more spread out and everything else gets even more squished up against the sides of the square. This reflects the fact that larger weights cause the sigmoid to sharpen and become more like a step function.</p>

<p>Notice also that the gridlines are all S-shaped, like the sigmoid curve (which owes its name to its peculiar S-shape). In fact, this is a consequence of using the sigmoid as the activation function. So does this nice “fish-eye” transformation depend on our choice of activation function? Let’s consider what would happen if we instead used a step function as the activation. Since the step function outputs only 0 or 1 for any input, every point in the input space would be mapped to one of four points in the new space: (0,0), (1,0), (0,1) and (1,1), the corners of the unit square. These correspond to the four possible output combinations of the two perceptrons in the first layer. Here’s what that looks like:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">perceptron_step</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">step</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>

<span class="n">nl_transform_step</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">perceptron_step</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">perceptron_step</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">x</span><span class="p">)],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="n">data_t_step</span> <span class="o">=</span> <span class="n">nl_transform_step</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">data_t_step_0</span> <span class="o">=</span> <span class="n">data_t_step</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">data_t_step_1</span> <span class="o">=</span> <span class="n">data_t_step</span><span class="p">[</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">n_gridlines</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">gridline_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">n_gridlines</span><span class="p">)</span>
<span class="n">gridline_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">n_gridlines</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">glx</span> <span class="ow">in</span> <span class="n">gridline_x</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">glx</span><span class="p">,</span> <span class="n">gridline_y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:rust'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gly</span> <span class="ow">in</span> <span class="n">gridline_y</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gridline_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">gly</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:deep blue'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #0'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #1'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$x_0$"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">glx</span> <span class="ow">in</span> <span class="n">gridline_x</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">nl_transform_step</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">glx</span><span class="p">,</span> <span class="n">gridline_y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:rust'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gly</span> <span class="ow">in</span> <span class="n">gridline_y</span><span class="p">:</span>
    <span class="n">g_t</span> <span class="o">=</span> <span class="n">nl_transform_step</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">gridline_x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_gridlines</span><span class="p">)</span> <span class="o">+</span> <span class="n">gly</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">g_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">g_t</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:deep blue'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_t_step_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_t_step_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #0'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_t_step_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_t_step_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #1'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Perceptron #0 output'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Perceptron #1 output'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/2017-12-17-seeing-like-a-perceptron/output_17_1.png" alt="png" /></p>

<p>What this means is that for each input data point, the second layer perceptron will receive one of those four corners as input and can use only that information to decide how to classify the point. This reduces the problem to a statistical average problem: what fraction of points in each corner belong to each class? We can conclude that if we were to use step activation, lots of information would certainly be lost, and thus sigmoid activation seems better. On the other hand, it is definitely possible for a two layer perceptron to achieve good accuracy on this particular data set even using step activation.</p>

<p>Ok, where are we now? Our hidden layer perceptrons found a non-linear transformation of the input space that yields a new space in which the data is more linearly separable than before. In this new space, we can easily eyeball a linear solution to the above classification, and a perceptron should have no trouble finding it either. Here’s one possible solution:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">decision_boundary_bias</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">*</span> <span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">bias</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>

<span class="n">t2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">b2</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.2</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_t_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_t_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_t_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_t_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #1'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t2</span><span class="p">,</span> <span class="n">decision_boundary_bias</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">t2</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:chocolate'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Perceptron #2 decision boundary'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Perceptron #0 output'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Perceptron #1 output'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/2017-12-17-seeing-like-a-perceptron/output_19_1.png" alt="png" /></p>

<p>We can visually verify that this final decision boundary will correctly classify all the points in the transformed space. What will this line have to look like in the input space? Scroll back up and take a look at how the data is laid out: the final decision boundary definitely can’t be a straight line! In fact it’s going to have to be very curved, pulling a tight right-angle turn near the point (0,0). We can visualise the final decision boundary by inverting the non-linear transformation that we applied to the data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">logit</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c"># Logit is the inverse of the sigmoid</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">perceptron_bias</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>

<span class="n">wt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">w0</span><span class="p">,</span> <span class="n">w1</span><span class="p">])</span>
<span class="n">wt_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">wt</span><span class="p">)</span>
<span class="n">nl_transform_inv</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">apply_along_axis</span><span class="p">(</span><span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">wt_inv</span><span class="p">,</span> <span class="n">logit</span><span class="p">(</span><span class="n">o</span><span class="p">)),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="n">t2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
<span class="n">db2</span> <span class="o">=</span> <span class="n">nl_transform_inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">t2</span><span class="p">,</span> <span class="n">decision_boundary_bias</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">t2</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #1'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="n">decision_boundary</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">t0</span><span class="p">),</span> <span class="s">'m'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Perceptron #0 decision boundary'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">decision_boundary</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">t1</span><span class="p">),</span> <span class="s">'g'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Perceptron #1 decision boundary'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">db2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">db2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'xkcd:chocolate'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Perceptron #2 decision boundary'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$x_0$"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/2017-12-17-seeing-like-a-perceptron/output_21_2.png" alt="png" /></p>

<p>I’ve also plotted the original decision boundaries so that you can compare them with the final decision boundary. From this vantage point, it’s clear that the final decision boundary is something like a weighted blend of the two boundaries from the first layer. This “blending” idea is usually the way people talk about multi-layer perceptrons, but to me it’s a little unsatisfying <sup id="fnref:ensembles"><a href="#fn:ensembles" class="footnote">2</a></sup>. It is hard to intuitively see how such a simple algorithm could be smart enough to figure out a way to shape and position this tricky curved decision boundary that separates the data in the input space. But if we look at what the perceptron in the second layer is actually “seeing”, it’s clear that it only has to solve the same problem that perceptrons always solve: find a line in space that separates the data. The space has changed, not the algorithm.</p>

<p>Alright, one last visualisation to complete the picture. The plot above shows the decision boundary of the final perceptron, which is really just a contour line along which it predicts a constant <script type="math/tex">\hat{y}</script> = 0.5. However, when building a predictive model you have the flexibility of choosing any activation threshold you want for the decision boundary. So what do the other contour lines look like in the original input space?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">perceptron_contour</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">y</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">perceptron_contour</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">perceptron_contour</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">),</span> <span class="n">perceptron_contour</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>
<span class="n">contours</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>
<span class="n">cs</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">contours</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">'</span><span class="si">%1.2</span><span class="s">f'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #0'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data_1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data_1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Class #1'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$x_0$"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="http://localhost:4000/assets/2017-12-17-seeing-like-a-perceptron/output_23_1.png" alt="png" /></p>

<p>Here you can see the <script type="math/tex">\hat{y}</script> = 0.5 decision boundary as before, along with a range of other possible choices. Note that in this scenario, everything above and to the right of a given boundary would be classified as class #1, and everything else as class #0. In the transformed space, these lines correspond to straight lines parallel to the decision boundary of the final perceptron.</p>

<p>To summarise what we have found: the perceptrons in the hidden layer of the neural network created a non-linear transformation that yielded a new space. This new space made the data easier to classify by zooming in on the areas of low confidence around the decision boundaries in the input space. The perceptron in the output layer effectively gets to work at higher magnification in the new space, and so it should be able to achieve better accuracy than any of the perceptrons in the hidden layer. Now imagine stacking layer upon layer of perceptrons in this manner, iteratively finding new transformations that zoom in further and further on the most difficult parts of the dataset, and you can begin to feel how complex the final decision boundary might become. And since we have algorithms for training perceptrons, they can find these transformation <em>by themselves</em>. I think that’s super cool, and it gets at the heart of what machine learning with neural networks really is: it’s not some magical black box hack, nor is it necessarily a simulation of brain activity. It’s a principled system for finding non-linear transformations that make the data easier to work with.</p>

<p><br /></p>

<div class="footnotes">
  <ol>
    <li id="fn:3blue1brown">
      <p>For a great introduction to the mathematics of transformations, I highly recommend 3Blue1Brown’s video series on the <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of Linear Algebra</a>.&nbsp;<a href="#fnref:3blue1brown" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:ensembles">
      <p>As an aside, if you take the blending analogy further then another way to think about a multi-layer perceptron is as a kind of <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble method</a>. An ensemble method is a technique for taking predictions from a bunch of lousy models and producing a kind of weighted average that is more correct more often than any of the individual models. One example is something called a random forest, where you create a group of decision trees, train each one on only a small subset of the data, and then use the trees’ collective predictions as a “vote” to decide how to classify each point. Another example is a technique called <a href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)">boosting</a> (the idea behind <a href="https://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a>), in which you train successive models on the data where the previous model made the most mistakes. In our case, we can think of the first layer of the perceptron as an ensemble of lousy perceptrons, except instead of just voting or averaging classifiers, we also do something a little like boosting: we focus in on the areas where the ensemble is least confident and train new models on those regions.&nbsp;<a href="#fnref:ensembles" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  
</article>

  <!-- </ul> -->

  <p class="rss-subscribe">subscribe <a href="/feed.xml">via RSS</a></p>

</div>

          </div>
          <div class="sidebar">
    
    
        <p>Math, machine learning, and generative art.</p>

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="sidebar-trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
            <a class="page-link" href="/papers/">Papers</a>
            
          
            
            
            <a class="page-link" href="/posts/">Posts</a>
            
          
            
            
          
        </div>
      </nav>
    
</div>
        </div>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!-- <h2 class="footer-heading">Tom Alcorn&#39;s Blog</h2> -->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <p>Math, machine learning, and generative art.</p>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/tdb-alcorn"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">tdb-alcorn</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <ul class="contact-list">
            
            <li>
              <!-- <a href="mailto:tdbalcorn@gmail.com"> -->
                tdbalcorn at gmail.com
              <!-- </a> -->
            </li>
            
        </ul>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
